# 퍼셉트론과 분류

## 목차
- 퍼셉트론
  - 퍼셉트론이란?
  - 단층 퍼셉트론
  - 다층 퍼셉트론
- 다중 분류


# 퍼셉트론

## 퍼셉트론이란?

[수치 예측과 이진 분류](https://github.com/Emotion-2019/gtg7784/blob/master/markdown/191104.md)

이 문서에 자세하게 설명되어있다.

## 단층 퍼셉트론

단층 퍼셉트론은 입력층(input layer)과 출력층(output layer)으로 구성된다. 입력층은 학습 벡터 또는 입력 벡터가 입력되는 계층으로써, 입력된 데이터는 출력층 뉴런으로 전달되어 활성함수에 따라 값이 출력된다.

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile23.uf.tistory.com%2Fimage%2F23573D3656C2D8421CF2FE" alt="출력층의 크기가 1인 단층 퍼셉트론" />

- 임계치: 어떠한 값이 활성화되기 위한 최소값을 임계치라고 한다.

- 가중치: 퍼셉트론의 학습 목표는 학습 벡터를 두 부류로 선형 분류하기 위한 선형 경계를 찾는 것이다. 가중치는 이러한 선형 경계의 방향성 또는 형태를 나타내는 값이다.

- 바이어스: 선형 경계의 절편을 나타내는 값으로써, 직선의 경우는 y절편을 나타낸다.

- net값: 입력값과 가중치의 곱을 모두 합한 값으로써, 기하학적으로 해석하면 선형 경계의 방정식과 같다.

- 활성홤수: 뉴런에서 계산된 net값이 임계치보다 크면 1을 출력하고, 임계치보다 작은 경우에는 0을 출력하는 함수이다. 이 정의는 단층 퍼셉트론에서만 유효하며, 다층 퍼셉트론에서는 다른 형태의 활성함수를 이용한다.

- 뉴런: 인공신경망을 구성하는 가장 작은 요소로써, net값이 임계치보다 크면 활성화되면서 1을 출력하고, 반대의 경우에는 비활성화되면서 0을 출력한다.

![뉴런의 구조](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile10.uf.tistory.com%2Fimage%2F261AC64E56C2D0E7241637)

## 다층 퍼셉트론

다층 퍼셉트론은 단층 퍼셉트론의 입력층(input layer)와 출력층(output layer) 사이에 은닉층(hidden layer)가 존재하여 층이 증가하는 것이다.

![다층 퍼셉트론](https://mblogthumb-phinf.pstatic.net/MjAxNzA2MTZfMzQg/MDAxNDk3NTc4MDc4NTU4.C0svtndk_igat9FquGev3il4HWs2mEKarZPuzI0kgMcg.75f89hjNck6eLQNZWQ4-QZiWNnArNKnuEXvxoL3n0aAg.PNG.samsjang/%EC%BA%A1%EC%B2%98.PNG?type=w2)

단층 퍼셉트론으로는, XOR 게이트와 같은 논리 연산을 할 수가 없다. 

다층 퍼셉트론은 단층 퍼셉트론과 유사한 구조를 가지고 있지만, 중간층과 각 unit 의 입출력 특성을 비선형으로 함으로써 네트워크의 능력을 향상시켜 단층 퍼셉트론의 단점을 극복했다.

은닉층이 여러개 있는 인공 신경망을 심층 신경망(Deep Neural Network)라 부르며, 심층 신경망을 학습하기 위한 알고리즘을 딥러닝(Deep Learning)이라 부른다.

다층 퍼셉토론에서는 입력층에서 전달되는 값이 은닉층의 모든 노드로 전달되며 은닉층의 모든 노드의 출력값 역시 출력층의 모든 노드로 전달된다.

이런 형식으로 값이 전달되는 것을 순전파(feedforward)라 한다. 입력층과 은닉층에 있는 한개의 노드만 볼 때, 하나의 단층 퍼셉트론으로 생각할 수 있다. 따라서 은닉층에 있는 각각의 노드는 퍼셉트론의 활성 함수라고 볼 수 있다.


# 다중 분류

이진 분류는 단순히 두개로 나누는 것 이라고 할 수 있으면, 다중 분류는 3개 혹은 그 이상으로 분류하는 것이다.

로지스틱 회귀는  0과 1.0 사이의 소수를 출력한다. 예를 들어 이메일 분류자의 로지스틱 회귀 출력이 0.8이면 이메일이 스팸일 확률이 80%이고 스팸이 아닐 확률이 20%임을 나타냅니다. 분명히 이메일이 스팸일 확률과 스팸이 아닐 확률의 합은 1.0이다.

소프트맥스는 이 아이디어를 다중 클래스 문제에 적용한다. 즉 소프트맥스가 다중 클래스 문제의 각 클래스에 소수 확률을 할당한다. 소수 확률의 합은 1.0이 되어야 한다. 이 제약조건을 추가하면 제약조건을 추가하지 않은 경우보다 더 빠르게 수렴을 학습할 수 있다.

![softmax](https://developers.google.com/machine-learning/crash-course/images/SoftmaxLayer.svg?hl=ko)

소프트맥스는 출력 레이어 바로 앞의 신경망 레이어를 통해 구현된다. 소프트맥스 레이어의 노드 수는 출력 레이어와 같아야 한다.