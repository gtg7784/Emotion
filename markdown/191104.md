# 수치 예측과 이진 분류

## 목차
- 수치 예측
  - 선형 회귀
  - 손실 함수, 경사 하강법
- 이진 분류
  - 퍼셉트론
  - 로지스틱 회귀
  - 시그모이드 함수

# 수치 예측

## 선형 회귀
2차원 좌표에 분포된 데이터를 1차원 직선 방정식을 통해 표현되지 않은 데이터를 예측하기 위한 분석 모델이다.

### 공식

![H(x)=Wx+b](https://latex.codecogs.com/gif.latex?H(x)=Wx&plus;b)

새로 들어온 데이터(x)에 대해서 어떤 값(H(x))이 될지 예측하는 것

### 용어
- **Error** 선형 회귀 모델의 직선과 실제 데이터와의 Y값의 차이다. />
![H(x) - y](https://latex.codecogs.com/gif.latex?H(x)&space;-&space;y)

- **Square Error** Error 를 제곱한 것 ![(H(x) - y)^2](https://latex.codecogs.com/gif.latex?(H(x)&space;-&space;y)^2)

Square Error을 사용하는 이유는, Error가 조금이라도 있다면, 값이 증폭되어 큰 값과 작은 값의 비교를 쉽게 할 수 있고, 경사하강법의 역전파(Backpropagation) 개념에서 계산에 용이하게 편미분된다.

- **Mean Square Error** Square Error 를 전부 다 더해서 그 값을 n으로 나눈 평균 값 ![1/n\sum (H(x) - y)^2](https://latex.codecogs.com/gif.latex?1/n\sum&space;(H(x)&space;-&space;y)^2)

## 경사 하강법

MSE(Mean Square Error)를 최소화 한 것이 선형 회귀 모델을 가장 잘 만든 것이다.

선형 회귀 모델을 최적화 시키는 알고리즘(MSE를 최소화 시키는 과정)을 경사하강법(Gradient Desent)이라고 한다.

먼저 이 알고리즘을 사용하지 전에, 알아야 할 개념이 Cost function이 MSE와 같은 것이다.

즉, 실제 값과 가설 값(예측 값)의 차이를 제곱하여 평균을 계산한 개념이 비용함수 (J(w, b), cost function) 이다.

그리고 이 Cost function 을 최저로 만든는 계념이 LMSE(Least Mean Square Error) 이다. 그리고 이렇게 cost function을 최저로 만든느 목적을 가진 함수이므로 목적 함수(object function)이라고도 한다.

경사하강법은 cost를 최소로 만드는 예측직선 H(x) = Wx 에서 최적의 W를 업데이트 하면서 찾아내는 과정이라고 볼수 있다. 

!["W:=W-\alpha \frac{\theta }{\theta W}cost(W)](https://latex.codecogs.com/gif.latex?W:=W-\alpha&space;\frac{\theta&space;}{\theta&space;W}cost(W))

  W : 첫번째 W으로서, 우리가 가장 처음에 초기화한 상수이다. gradient를 태워서 cost를 최소로 만드는 W로, 점점 업데이트 된다.

  α : learning rate로서, 학습 속도를 조절하는 상수이다.

  ![\alpha \frac{\theta }{\theta W}cost(W)](https://latex.codecogs.com/gif.latex?\alpha&space;\frac{\theta&space;}{\theta&space;W}cost(W)) : cost 즉, MSE를 W로 편미분한 것.
  
## 예시 코드

~~~
import tensorflow as tf

# 선형 회귀 모델(Wx + b)을 정의합니다.
W = tf.Variable(tf.random_normal(shape=[1]), name='W')
b = tf.Variable(tf.random_normal(shape=[1]), name='b')
x = tf.placeholder(tf.float32, name='x')

linear_model = W * x + b

# True Value를 입력받기위한 플레이스홀더를 정의합니다.
y = tf.placeholder(tf.float32, name='y')

# 평균제곱오차(MSE) 손실 함수를 정의합니다.
loss = tf.reduce_mean(tf.square(linear_model - y))  #MSE 손실함수 \mean{(y'-y)^2}
# 텐서보드를 위한 요약 정보(scalar)를 정의합니다.
tf.summary.scalar('loss', loss)

# 최적화를 위한 그라디언트 디센트 옵티마이저를 정의합니다.
optimizer = tf.train.GradientDescentOptimizer(0.01)
train_step = optimizer.minimize(loss)

# 트레이닝을 위한 입력값과 출력값을 준비합니다.
x_train = [1, 2, 3, 4]
y_train = [2, 4, 6, 8]

# 세션을 실행하고 파라미터(W,b)를 normal distribution에서 추출한 임의의 값으로 초기화합니다.
sess = tf.Session()
sess.run(tf.global_variables_initializer())
# 텐서보드 요약 정보들을 하나로 합칩니다.
merged = tf.summary.merge_all()
# 스템바다 텐서보드 요약 정보들을 저장할 폴더 경로를 설정합니다.
tensorboard_writer = tf.summary.FileWriter('./tensorboard_log', sess.graph)

# 경사하갈법을 1000번 수행합니다.
for i in range(1000):
    sess.run(train_step, feed_dict={x: x_train, y: y_train})

    # 스텝마다 텐서보드 요약 정보 값들을 계산해서 지정된 경로('./tensorboard.log')에 저장합니다.
    summary = sess.run(merged, feed_dict={x: x_train, y: y_train})
    tensorboard_writer.add_summary(summary, i)

# 테스트를 위한 입력값을 준비합니다.
x_test = [3.5, 5, 5.5, 6]
# 테스트 데이터를 이용해 학습된 선형 회귀 모델이 데이터릐 경향성(y=2x)을 잘 학습했는지 측정합니다.
# 에상되는 참 값: [7, 10, 11, 12]
print(sess.run(linear_model, feed_dict={x: x_test}))

sess.close()
~~~


# 이진 분류

## 퍼셉트론

퍼셉트론은 다수의 신호(Input)를 입력 받아서 하나의 신호(Output)를 출력 시킨다.

![퍼셉트론](https://t1.daumcdn.net/cfile/tistory/23573D3656C2D8421C)

가중치(weight)는 각각의 입력신호에 부여되어 입력신호와의 계산을 하고 신호의 총합이 정해진 임계값(θ)을 넘었을 때 1을 출력한다 또한 θ를 넘지 못하면 0 또는 -1을 출력한다.

(이를 뉴련의 활성화(activation) 으로도 표현)

각 입력신호에는 고유한 weight가 부여되며 weight가 클수록 해당 신호가 중요하다고 볼 수 있다.

여기서 기계학습이 하는 일은 이 weight의 값을 정하는 작업이라고 할 수 있다.

학습 알고리즘에 따라 방식이 다를 뿐 이 weight를 만들어내는 것이 학습이라는 차원에서는  모두 같다고 할 수 있다

퍼셉트론의 출력 값은 앞에서 말했듯이 1 또는 0(or -1)이기 때문에 선형 분류(linear classifier) 모형이라고도 볼 수 있다

## 로지스틱 회귀

로지스틱 회귀를 하기 전에, 먼저 Logistic function 과 Odds를 알아야 한다.

이진 분류를 선형 회귀로 진행하게 되면, Y값은 0또는 1이 나와야 하기 때문에 선형 회귀로는 하기가 어렵다. 그래서 step function과 비슷하게 곡선으로 하는 것이 Logistic function이다.

Odds 이란 임의의 사건 A가 발생하지 않을 확률 대비 일어날 확률의 비율을 뜻하는 개념입니다.

그 식은 다음과 같다. 

![odds = \frac{P(A))}{P(A^c)} = \frac{P(A)}{1-P(A)}](https://latex.codecogs.com/gif.latex?odds&space;=&space;\frac{P(A))}{P(A^c)}&space;=&space;\frac{P(A)}{1-P(A)})

만약 P(A)가 1에 가까울 수록 Odds 는 치솟을 것이다. 반대로 P(A)가 0이라면 0이 될 것이다. 

바꿔 말하면 Odds가 커질수록 사건 A가 발생할 확률이 커진다고 이해해도 된다. 

## 시그모이드 함수

딥 러닝에서 사용되는 다양한 활성 함수(Activation function) 중 대표적인 함수인 Sigmoid function이다.

Sigmoid function는 S자와 유사한 완만한 Sigmoid 커브 형태를 보이는 함수이다 또한 대표적인 Logistic Function이다.

Sigmoid function는 모든 실수 입력 값을 0보다 크고 1보다 작은 미분 가능한 수로 변환하는 특징을 가지고 있다.

모든 입력에 대하여 sigmoid는 S와 같은 형태로 미분 가능한 0~1 사이의 값을 반환하기에 Logistic Classification과 같은 분류 문제의 가설과 비용 함수(Cost Function)에 많이 사용된다.

Sigmoid 함수는 다음과 같이 정의 된다.


![y = \frac{1}{1 + e^-^x}](https://latex.codecogs.com/gif.latex?y&space;=&space;\frac{1}{1&space;&plus;&space;e^-^x})

### 예제 코드
~~~
import math

def sigmoid(x, deff=False):
  if deff:
    return sigmoid(x)*(1-sigmoid(x))
  else:
    return 1 / (1 + math.exp(-x))
~~~